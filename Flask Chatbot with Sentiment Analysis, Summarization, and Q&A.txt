# -*- coding: utf-8 -*-
"""Chapter4_Solution.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k5PXA3jONyjIb606GEd9Y_gJWPyD5WCm

# LoRA Fine-Tuning with FLAN-T5

## Introduction
In this notebook, we will perform LoRA fine-tuning on the FLAN-T5-base model using the WMT16 dataset. We will replace the dense layers with LoRA layers and fine-tune the model for sentiment classification.
"""

!pip install transformers tensorflow datasets tensorflow_addons

"""## Load and Preprocess the  Dataset"""

from datasets import load_dataset

# Load the WMT16 English-German dataset
dataset = load_dataset('wmt16', 'de-en')

# Display an example
print(dataset['train'][0])

import tensorflow as tf
from transformers import AutoTokenizer

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')

# Preprocess the dataset for input into the model
def preprocess_data(examples):
    inputs = [f'Translate English to German: {example["en"]}' for example in examples['translation']]
    targets = [example['de'] for example in examples['translation']]
    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding='max_length', return_tensors='tf')
    labels = tokenizer(targets, max_length=128, truncation=True, padding='max_length', return_tensors='tf').input_ids
    model_inputs['labels'] = labels
    decoder_inputs = tokenizer(targets, max_length=128, truncation=True, padding="max_length")
    model_inputs["decoder_input_ids"] = decoder_inputs["input_ids"]
    return model_inputs


train_dataset = dataset['train'].select(range(20000)).map(preprocess_data, batched=True)
test_dataset = dataset['test'].select(range(1000)).map(preprocess_data, batched=True)

train_dataset = train_dataset.to_tf_dataset(
    columns=['input_ids', 'attention_mask', 'decoder_input_ids'],
    label_cols=['labels'],
    shuffle=True,
    batch_size=128,
    collate_fn=None
)

test_dataset = test_dataset.to_tf_dataset(
    columns=['input_ids', 'attention_mask', 'decoder_input_ids'],
    label_cols=['labels'],
    shuffle=False,
    batch_size=128,
    collate_fn=None
)



"""## Load the Pre-trained FLAN-T5 Model and Modify"""

from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer
import tensorflow_addons as tfa
from tensorflow.keras.layers import Dense

# Load the model
model = TFAutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-small')

# Replace the dense layers with LoRA layers
class LoRALayer(tf.keras.layers.Layer):
    def __init__(self, dense, rank=4):
        super().__init__()
        self.dense = dense
        self.rank = rank

    def build(self, input_shape):
        self.w_a = self.add_weight(shape=(input_shape[-1], self.rank),
                                   initializer='random_normal',
                                   trainable=True, name='w_a')
        self.w_b = self.add_weight(shape=(self.rank, self.dense.units),
                                   initializer='random_normal',
                                   trainable=True, name='w_b')

    def call(self, inputs):
        original_output = self.dense(inputs)
        lora_output = tf.matmul(tf.matmul(inputs, self.w_a), self.w_b)
        self.dense.trainable = False
        return original_output + lora_output

model.summary()

import tf_keras
model.get_layer('encoder').trainable = False
model.get_layer('shared').trainable = False
model.get_layer('decoder').trainable = False
model.layers[3] = LoRALayer(model.get_layer('lm_head'))

"""## Count Trainable and Non-Trainable Parameters"""

trainable_params = tf.reduce_sum([tf.reduce_prod(v.shape) for v in model.trainable_variables])
non_trainable_params = tf.reduce_sum([tf.reduce_prod(v.shape) for v in model.non_trainable_variables])

print(f'Trainable parameters: {trainable_params.numpy()}')
print(f'Non-trainable parameters: {non_trainable_params.numpy()}')

"""## Compile and Train the Model"""

model.summary()

"""## Train the Model"""

# Compile the model
model.compile(optimizer=tf_keras.optimizers.Adam(learning_rate=1e-3),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))

# Train the model
model.fit(train_dataset, validation_data=test_dataset, epochs=3)

"""## Evaluate the Model"""

# Evaluate the model
model.evaluate(test_dataset)

"""## Verify Translations with BLEU"""

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import nltk
nltk.download('punkt')

# Function to calculate BLEU score
def calculate_bleu(reference, hypothesis):
    reference_tokens = [nltk.word_tokenize(reference)]
    hypothesis_tokens = nltk.word_tokenize(hypothesis)
    bleu_score = sentence_bleu(reference_tokens, hypothesis_tokens, smoothing_function=SmoothingFunction().method4)
    return bleu_score

# Function to translate and evaluate
def translate_and_evaluate(dataset):
    bleu_scores = []
    batch = next(iter(dataset))
    inputs = batch[0]['input_ids']
    references = batch[1]
    outputs = model.generate(inputs, max_length=128, num_beams=4, early_stopping=True)
    for i in range(len(inputs)):
        reference = tokenizer.decode(references[i], skip_special_tokens=True)
        hypothesis = tokenizer.decode(outputs[i], skip_special_tokens=True)
        bleu_score = calculate_bleu(reference, hypothesis)
        bleu_scores.append(bleu_score)
    return sum(bleu_scores) / len(bleu_scores)

# Evaluate on the validation set
average_bleu_score = translate_and_evaluate(test_dataset)
print(f'Average BLEU score on validation set: {average_bleu_score}')

